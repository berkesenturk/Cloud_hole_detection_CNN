{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dc576ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.utils import (\n",
    "    NetCDFToZarrConverter\n",
    ")\n",
    "raw_data_path = \"../data/raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a8f47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 19:37:57,178 - INFO - Analyzing NetCDF files matching: ../data/raw/seviri/hrv_lr2*.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 19:37:57,228 - INFO - Found 56 files\n",
      "/home/plato/dl_cloudhole/dl_cloudhole/src/utils.py:69: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  dims = dict(ds.dims)\n",
      "2026-02-01 19:38:04,896 - INFO - Calculated chunk size: 19.97 MB | chunks={'time': 261, 'y': 92, 'x': 109}\n",
      "2026-02-01 19:38:04,898 - INFO - Analysis complete: {'num_files': 56, 'files': ['hrv_lr200401.nc', 'hrv_lr200411.nc', 'hrv_lr200412.nc', 'hrv_lr200501.nc', 'hrv_lr200511.nc', 'hrv_lr200512.nc', 'hrv_lr200601.nc', 'hrv_lr200611.nc', 'hrv_lr200612.nc', 'hrv_lr200701.nc', 'hrv_lr200711.nc', 'hrv_lr200712.nc', 'hrv_lr200801.nc', 'hrv_lr200811.nc', 'hrv_lr200812.nc', 'hrv_lr200901.nc', 'hrv_lr200911.nc', 'hrv_lr200912.nc', 'hrv_lr201001.nc', 'hrv_lr201011.nc', 'hrv_lr201012.nc', 'hrv_lr201101.nc', 'hrv_lr201111.nc', 'hrv_lr201112.nc', 'hrv_lr201201.nc', 'hrv_lr201202.nc', 'hrv_lr201211.nc', 'hrv_lr201212.nc', 'hrv_lr201301.nc', 'hrv_lr201302.nc', 'hrv_lr201311.nc', 'hrv_lr201312.nc', 'hrv_lr201401.nc', 'hrv_lr201402.nc', 'hrv_lr201411.nc', 'hrv_lr201412.nc', 'hrv_lr201501.nc', 'hrv_lr201502.nc', 'hrv_lr201511.nc', 'hrv_lr201512.nc', 'hrv_lr201601.nc', 'hrv_lr201602.nc', 'hrv_lr201611.nc', 'hrv_lr201612.nc', 'hrv_lr201701.nc', 'hrv_lr201702.nc', 'hrv_lr201711.nc', 'hrv_lr201712.nc', 'hrv_lr201801.nc', 'hrv_lr201802.nc', 'hrv_lr201811.nc', 'hrv_lr201812.nc', 'hrv_lr201901.nc', 'hrv_lr201902.nc', 'hrv_lr201911.nc', 'hrv_lr201912.nc'], 'dimensions': {'time': 52928, 'y': 92, 'x': 109}, 'variables': ['hrv'], 'coordinates': ['lat', 'lon', 'time'], 'dtype_size': 8, 'recommended_chunks': {'time': 261, 'y': 92, 'x': 109}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time': 261, 'y': 92, 'x': 109}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling only performed for time dimension \n",
    "# 20MB per chunk is aimed\n",
    "\n",
    "converter = NetCDFToZarrConverter()\n",
    "\n",
    "chunk_analysis = converter.analyze_netcdf_files(\"../data/raw/seviri/hrv_lr2*.nc\")\n",
    "\n",
    "chunk_analysis[\"recommended_chunks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04c2f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "input_file_pattern=\"../data/raw/seviri/hrv_lr2*.nc\"\n",
    "\n",
    "files = sorted(Path().glob(input_file_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f2aeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 19:38:04,933 - INFO - Converting multiple files to single Zarr: ../data/raw/seviri/hrv_lr2*.nc\n",
      "2026-02-01 19:38:04,936 - INFO - Found 56 files to convert\n",
      "2026-02-01 19:38:06,209 - INFO - Writing consolidated Zarr with chunks: {'time': 261, 'y': 92, 'x': 109}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.955028712749481 GB dataset\n",
      "Frozen({'time': (261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 206), 'y': (92,), 'x': (109,)})\n",
      "[########################################] | 100% Completed | 39.96 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 19:38:46,669 - INFO - Successfully created consolidated Zarr at ../data/processed/seviri/hrv_lr2004_2019.zarr\n",
      "2026-02-01 19:38:46,671 - INFO - \n",
      "============================================================\n",
      "2026-02-01 19:38:46,673 - INFO - Zarr Store Information: ../data/processed/seviri/hrv_lr2004_2019.zarr\n",
      "2026-02-01 19:38:46,674 - INFO - ============================================================\n",
      "2026-02-01 19:38:46,675 - INFO - /\n",
      " ├── hrv (52928, 92, 109) float64\n",
      " ├── lat (92, 109) float64\n",
      " ├── lon (92, 109) float64\n",
      " └── time (52928,) int64\n",
      "2026-02-01 19:38:46,678 - INFO - ============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Issue:Segmentation fault (core dumped)\n",
    "- Reason: parallel=True, chunks='auto' on xr.open_mfdataset\n",
    "- Solution: set parallel false and chunks to None\n",
    "\"\"\"\n",
    "\n",
    "converter.convert_multiple_files_to_single_zarr(\n",
    "    file_pattern = input_file_pattern,\n",
    "    output_path = f\"../data/processed/seviri/hrv_lr{files[0].as_posix()[-9:-5]}_{files[-1].as_posix()[-9:-5]}.zarr\",\n",
    "    custom_chunks = chunk_analysis[\"recommended_chunks\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03d8c931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 12.788631439208984, Std: 7.1760993003845215\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.data.datasets import CloudHoleDataset\n",
    "first_year, last_year = files[0].as_posix()[-9:-5], files[-1].as_posix()[-9:-5]\n",
    "\n",
    "processed_data_path = \"../data/processed\"\n",
    "\n",
    "dataset = CloudHoleDataset(\n",
    "    labels=f\"{processed_data_path}/julia_labels.csv\",\n",
    "    data_dir=f\"{processed_data_path}/seviri/hrv_lr{first_year}_{last_year}.zarr\",\n",
    "    years=range(int(first_year), int(last_year)+1)\n",
    ")\n",
    "\n",
    "mean, std = dataset.mean, dataset.std\n",
    "\n",
    "print(f\"Mean: {mean}, Std: {std}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
